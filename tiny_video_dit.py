# -*- coding: utf-8 -*-
"""Tiny Video DiT

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p4okvLmSJXenVDJc8F4qVkZLSfY_qZXh
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

#code based on sinusoid embedding code from towards data science article and original ViT encodings

def sine_encoding(patches, patch_size):
  def get_theta(i):
    return [i / np.power(10000, 2 * (j//2) / patch_size) for j in range(patch_size)]

  ret_table = np.array([get_theta(i) for i in range(patches)])
  ret_table[:, 0::2] = np.sin(ret_table[:, 0::2])
  ret_table[:, 1::2] = np.cos(ret_table[:, 1::2])
  return torch.FloatTensor(ret_table)

def timestep_embedding(timesteps, dim):
    half_dim = dim // 2
    emb = np.log(10000) / (half_dim - 1)
    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)
    emb = timesteps[:, None] * emb[None, :]
    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)
    return emb

def patchify_video(video, patch_size):
  b, t, h, w, c = video.shape
  pt, ph, pw = patch_size
  num_t = t // pt
  num_h = h // ph
  num_w = w // pw
  num_patches = num_t * num_h * num_w
  patch_size = pt * ph * pw * c
  patched_video = video.reshape(b, num_t, pt, num_h, ph, num_w, pw, c)
  reordered_patched_video = patched_video.permute(0, 1, 3, 5, 2, 4, 6, 7)
  ret_patches = reordered_patched_video.reshape(b, num_patches, patch_size)
  return ret_patches

def patchify_image(image, patch_size):
  b, h, w, c = image.shape
  pt, ph, pw = patch_size
  num_h = h // ph
  num_w = w // pw
  num_patches = num_h * num_w
  patch_size = ph * pw * c
  patched_image = image.reshape(b, num_h, ph, num_w, pw, c)
  reordered_patched_image = patched_image.permute(0, 1, 3, 2, 4, 5)
  ret_patches = reordered_patched_image.reshape(b, num_patches, patch_size)
  return ret_patches
class VideoDiT(nn.Module):
  def __init__(self, channels=3, patch_size=(2, 8, 8), resolution=(64, 64), frames=16, embedding_size=512, depth=6, heads=8):
    super().__init__()
    self.channels = channels
    self.patch_size = patch_size
    self.resolution = resolution
    self.frames = frames
    self.embedding_size = embedding_size
    self.depth = depth
    self.heads = heads
    self.num_image_patches = (resolution[0] // patch_size[1]) * (resolution[1] // patch_size[2])

    self.num_patches = (frames // patch_size[0]) * (resolution[0] // patch_size[1]) * (resolution[1] // patch_size[2])
    self.patch_embedding = nn.Linear(self.patch_size[0] * self.patch_size[1] * self.patch_size[2] * self.channels, self.embedding_size)
    self.condition_embedding = nn.Linear(self.num_image_patches * self.channels, self.embedding_size)

    self.pos_encoding = sine_encoding(self.num_patches, self.embedding_size)
    self.transformer_layers = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=self.embedding_size, nhead=heads, dim_feedforward=4*embedding_size, batch_first=True), num_layers=depth)
    self.output_layer = nn.Linear(self.embedding_size, self.channels * self.patch_size[0] * self.patch_size[1] * self.patch_size[2])
    self.time_embedding = nn.Linear(self.embedding_size, self.embedding_size)

  def forward(self, video, timesteps):
    b, t, h, w, c = video.shape
    patches = patchify_video(video, self.patch_size)
    x = self.patch_embedding(patches)
    enc = self.pos_encoding[:x.shape[1]].to(x.device)
    x_enc = x + enc.unsqueeze(0)

    time_enc = timestep_embedding(timesteps, self.embedding_size).to(x_enc.device)
    time_enc = self.time_embedding(time_enc)
    x_enc = x_enc + time_enc.unsqueeze(1)

    conditioning_image = video[:, 0, :, :, :]
    cond_image_patches = patchify_image(conditioning_image, self.patch_size)
    num_cond_image_patches = cond_image_patches.shape[1]
    cond_image_pos = sine_encoding(num_cond_image_patches, self.embedding_size).to(x_enc.device)
    cond_image_enc = self.condition_embedding(cond_image_patches) + cond_image_pos.unsqueeze(0)
    print("cond_image_enc shape:", cond_image_enc.shape)
    print("x_enc shape:", x_enc.shape)
    x_enc = torch.cat([cond_image_enc, x_enc], dim=1)
    x_trans = self.transformer_layers(x_enc)
    x_trans = x_trans[:, cond_image_enc.shape[1]:]
    output = self.output_layer(x_trans)
    output = output.view(b, -1, *self.patch_size, c)
    return output